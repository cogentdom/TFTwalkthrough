{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a361aae",
   "metadata": {},
   "source": [
    "### Demand forecasting with the Temporal Fusion Transformer\n",
    "\n",
    "In this tutorial, we will train the <b>TemporalFusionTransformer</b> on a very small dataset to demonstrate that it even does a good job on only 20k samples. Generally speaking, it is a large model and will therefore perform much better with more data.\n",
    "\n",
    "Our example is a demand forecast from the Stallion kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bec2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812852fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6990f3",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "First, we need to transform our time series into a pandas dataframe where each row can be identified with a time step and a time series. Fortunately, most datasets are already in this format. For this tutorial, we will use the Stallion dataset from Kaggle describing sales of various beverages. Our task is to make a six-month forecast of the sold volume by stock keeping units (SKU), that is products, sold by an agency, that is a store. There are about 21 000 monthly historic sales records. In addition to historic sales we have information about the sales price, the location of the agency, special days such as holidays, and volume sold in the entire industry.\n",
    "\n",
    "The dataset is already in the correct format but misses some important features. Most importantly, we need to add a time index that is incremented by one for each time step. Further, it is beneficial to add date features, which in this case means extracting the month from the date record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f615f945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>sku</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>...</th>\n",
       "      <th>football_gold_cup</th>\n",
       "      <th>beer_capital</th>\n",
       "      <th>music_fest</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>month</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>avg_volume_by_sku</th>\n",
       "      <th>avg_volume_by_agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Agency_25</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>25.845238</td>\n",
       "      <td>1264.162234</td>\n",
       "      <td>1152.473405</td>\n",
       "      <td>111.688829</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>8.835008</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.678062</td>\n",
       "      <td>1225.306376</td>\n",
       "      <td>99.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Agency_29</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>8.7480</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>498567142</td>\n",
       "      <td>762225057</td>\n",
       "      <td>27.584615</td>\n",
       "      <td>1316.098485</td>\n",
       "      <td>1296.804924</td>\n",
       "      <td>19.293561</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.465966</td>\n",
       "      <td>177</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2.168825</td>\n",
       "      <td>1634.434615</td>\n",
       "      <td>11.397086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19532</th>\n",
       "      <td>Agency_47</td>\n",
       "      <td>SKU_01</td>\n",
       "      <td>4.9680</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>454252482</td>\n",
       "      <td>789624076</td>\n",
       "      <td>30.665957</td>\n",
       "      <td>1269.250000</td>\n",
       "      <td>1266.490490</td>\n",
       "      <td>2.759510</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.217413</td>\n",
       "      <td>322</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1.603017</td>\n",
       "      <td>2625.472644</td>\n",
       "      <td>48.295650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Agency_53</td>\n",
       "      <td>SKU_07</td>\n",
       "      <td>21.6825</td>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>480693900</td>\n",
       "      <td>791658684</td>\n",
       "      <td>29.197727</td>\n",
       "      <td>1193.842373</td>\n",
       "      <td>1128.124395</td>\n",
       "      <td>65.717978</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>beer_capital</td>\n",
       "      <td>-</td>\n",
       "      <td>5.504745</td>\n",
       "      <td>240</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>3.076505</td>\n",
       "      <td>38.529107</td>\n",
       "      <td>2511.035175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>Agency_17</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>960.5520</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>515468092</td>\n",
       "      <td>871204688</td>\n",
       "      <td>23.608120</td>\n",
       "      <td>1338.334248</td>\n",
       "      <td>1232.128069</td>\n",
       "      <td>106.206179</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>music_fest</td>\n",
       "      <td>7.935699</td>\n",
       "      <td>259</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>6.867508</td>\n",
       "      <td>2143.677462</td>\n",
       "      <td>396.022140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>Agency_05</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>1184.6535</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>425528909</td>\n",
       "      <td>734443953</td>\n",
       "      <td>28.668254</td>\n",
       "      <td>1369.556376</td>\n",
       "      <td>1161.135214</td>\n",
       "      <td>208.421162</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>15.218151</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>7.077206</td>\n",
       "      <td>1566.643589</td>\n",
       "      <td>1881.866367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19204</th>\n",
       "      <td>Agency_11</td>\n",
       "      <td>SKU_05</td>\n",
       "      <td>5.5593</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>623319783</td>\n",
       "      <td>1049868815</td>\n",
       "      <td>31.915385</td>\n",
       "      <td>1922.486644</td>\n",
       "      <td>1651.307674</td>\n",
       "      <td>271.178970</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>14.105636</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>1.715472</td>\n",
       "      <td>1385.225478</td>\n",
       "      <td>109.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>Agency_48</td>\n",
       "      <td>SKU_04</td>\n",
       "      <td>4275.1605</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>509281531</td>\n",
       "      <td>892192092</td>\n",
       "      <td>26.767857</td>\n",
       "      <td>1761.258209</td>\n",
       "      <td>1546.059670</td>\n",
       "      <td>215.198539</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>music_fest</td>\n",
       "      <td>12.218455</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8.360577</td>\n",
       "      <td>1757.950603</td>\n",
       "      <td>1925.272108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>Agency_07</td>\n",
       "      <td>SKU_21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>544203593</td>\n",
       "      <td>761469815</td>\n",
       "      <td>28.987755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>-18.420681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2418.719550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>Agency_21</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>46.3608</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>589969396</td>\n",
       "      <td>940912941</td>\n",
       "      <td>32.478910</td>\n",
       "      <td>1675.922116</td>\n",
       "      <td>1413.571789</td>\n",
       "      <td>262.350327</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>15.654088</td>\n",
       "      <td>181</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>3.836454</td>\n",
       "      <td>2034.293024</td>\n",
       "      <td>109.381800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          agency     sku     volume       date  industry_volume  soda_volume  \\\n",
       "291    Agency_25  SKU_03     0.5076 2013-01-01        492612703    718394219   \n",
       "871    Agency_29  SKU_02     8.7480 2015-01-01        498567142    762225057   \n",
       "19532  Agency_47  SKU_01     4.9680 2013-09-01        454252482    789624076   \n",
       "2089   Agency_53  SKU_07    21.6825 2013-10-01        480693900    791658684   \n",
       "9755   Agency_17  SKU_02   960.5520 2015-03-01        515468092    871204688   \n",
       "7561   Agency_05  SKU_03  1184.6535 2014-02-01        425528909    734443953   \n",
       "19204  Agency_11  SKU_05     5.5593 2017-08-01        623319783   1049868815   \n",
       "8781   Agency_48  SKU_04  4275.1605 2013-03-01        509281531    892192092   \n",
       "2540   Agency_07  SKU_21     0.0000 2015-10-01        544203593    761469815   \n",
       "12084  Agency_21  SKU_03    46.3608 2017-04-01        589969396    940912941   \n",
       "\n",
       "       avg_max_temp  price_regular  price_actual    discount  ...  \\\n",
       "291       25.845238    1264.162234   1152.473405  111.688829  ...   \n",
       "871       27.584615    1316.098485   1296.804924   19.293561  ...   \n",
       "19532     30.665957    1269.250000   1266.490490    2.759510  ...   \n",
       "2089      29.197727    1193.842373   1128.124395   65.717978  ...   \n",
       "9755      23.608120    1338.334248   1232.128069  106.206179  ...   \n",
       "7561      28.668254    1369.556376   1161.135214  208.421162  ...   \n",
       "19204     31.915385    1922.486644   1651.307674  271.178970  ...   \n",
       "8781      26.767857    1761.258209   1546.059670  215.198539  ...   \n",
       "2540      28.987755       0.000000      0.000000    0.000000  ...   \n",
       "12084     32.478910    1675.922116   1413.571789  262.350327  ...   \n",
       "\n",
       "       football_gold_cup  beer_capital  music_fest discount_in_percent  \\\n",
       "291                    -             -           -            8.835008   \n",
       "871                    -             -           -            1.465966   \n",
       "19532                  -             -           -            0.217413   \n",
       "2089                   -  beer_capital           -            5.504745   \n",
       "9755                   -             -  music_fest            7.935699   \n",
       "7561                   -             -           -           15.218151   \n",
       "19204                  -             -           -           14.105636   \n",
       "8781                   -             -  music_fest           12.218455   \n",
       "2540                   -             -           -            0.000000   \n",
       "12084                  -             -           -           15.654088   \n",
       "\n",
       "      timeseries time_idx month log_volume avg_volume_by_sku  \\\n",
       "291          228        0     1  -0.678062       1225.306376   \n",
       "871          177       24     1   2.168825       1634.434615   \n",
       "19532        322        8     9   1.603017       2625.472644   \n",
       "2089         240        9    10   3.076505         38.529107   \n",
       "9755         259       26     3   6.867508       2143.677462   \n",
       "7561          21       13     2   7.077206       1566.643589   \n",
       "19204         17       55     8   1.715472       1385.225478   \n",
       "8781         151        2     3   8.360577       1757.950603   \n",
       "2540         300       33    10 -18.420681          0.000000   \n",
       "12084        181       51     4   3.836454       2034.293024   \n",
       "\n",
       "      avg_volume_by_agency  \n",
       "291              99.650400  \n",
       "871              11.397086  \n",
       "19532            48.295650  \n",
       "2089           2511.035175  \n",
       "9755            396.022140  \n",
       "7561           1881.866367  \n",
       "19204           109.699200  \n",
       "8781           1925.272108  \n",
       "2540           2418.719550  \n",
       "12084           109.381800  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "data = get_stallion_data()\n",
    "\n",
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "\n",
    "# we want to encode special days as one variable and thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a9f32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>avg_population_2017</th>\n",
       "      <th>avg_yearly_household_income_2017</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>avg_volume_by_sku</th>\n",
       "      <th>avg_volume_by_agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21000.000000</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.00000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1492.403982</td>\n",
       "      <td>5.439214e+08</td>\n",
       "      <td>8.512000e+08</td>\n",
       "      <td>28.612404</td>\n",
       "      <td>1451.536344</td>\n",
       "      <td>1267.347450</td>\n",
       "      <td>184.374146</td>\n",
       "      <td>1.045065e+06</td>\n",
       "      <td>151073.494286</td>\n",
       "      <td>10.574884</td>\n",
       "      <td>174.50000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>2.464118</td>\n",
       "      <td>1492.403982</td>\n",
       "      <td>1492.403982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2711.496882</td>\n",
       "      <td>6.288022e+07</td>\n",
       "      <td>7.824340e+07</td>\n",
       "      <td>3.972833</td>\n",
       "      <td>683.362417</td>\n",
       "      <td>587.757323</td>\n",
       "      <td>257.469968</td>\n",
       "      <td>9.291926e+05</td>\n",
       "      <td>50409.593114</td>\n",
       "      <td>9.590813</td>\n",
       "      <td>101.03829</td>\n",
       "      <td>17.318515</td>\n",
       "      <td>8.178218</td>\n",
       "      <td>1051.790829</td>\n",
       "      <td>1328.239698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.130518e+08</td>\n",
       "      <td>6.964015e+08</td>\n",
       "      <td>16.731034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3121.690141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.227100e+04</td>\n",
       "      <td>90240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-18.420681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.272388</td>\n",
       "      <td>5.090553e+08</td>\n",
       "      <td>7.890880e+08</td>\n",
       "      <td>25.374816</td>\n",
       "      <td>1311.547158</td>\n",
       "      <td>1178.365653</td>\n",
       "      <td>54.935108</td>\n",
       "      <td>6.018900e+04</td>\n",
       "      <td>110057.000000</td>\n",
       "      <td>3.749628</td>\n",
       "      <td>87.00000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>2.112923</td>\n",
       "      <td>932.285496</td>\n",
       "      <td>113.420250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>158.436000</td>\n",
       "      <td>5.512000e+08</td>\n",
       "      <td>8.649196e+08</td>\n",
       "      <td>28.479272</td>\n",
       "      <td>1495.174592</td>\n",
       "      <td>1324.695705</td>\n",
       "      <td>138.307225</td>\n",
       "      <td>1.232242e+06</td>\n",
       "      <td>131411.000000</td>\n",
       "      <td>8.948990</td>\n",
       "      <td>174.50000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>5.065351</td>\n",
       "      <td>1402.305264</td>\n",
       "      <td>1730.529771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1774.793475</td>\n",
       "      <td>5.893715e+08</td>\n",
       "      <td>9.005551e+08</td>\n",
       "      <td>31.568405</td>\n",
       "      <td>1725.652080</td>\n",
       "      <td>1517.311427</td>\n",
       "      <td>272.298630</td>\n",
       "      <td>1.729177e+06</td>\n",
       "      <td>206553.000000</td>\n",
       "      <td>15.647058</td>\n",
       "      <td>262.00000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>7.481439</td>\n",
       "      <td>2195.362302</td>\n",
       "      <td>2595.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22526.610000</td>\n",
       "      <td>6.700157e+08</td>\n",
       "      <td>1.049869e+09</td>\n",
       "      <td>45.290476</td>\n",
       "      <td>19166.625000</td>\n",
       "      <td>4925.404000</td>\n",
       "      <td>19166.625000</td>\n",
       "      <td>3.137874e+06</td>\n",
       "      <td>247220.000000</td>\n",
       "      <td>226.740147</td>\n",
       "      <td>349.00000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>10.022453</td>\n",
       "      <td>4332.363750</td>\n",
       "      <td>5884.717375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             volume  industry_volume   soda_volume  avg_max_temp  \\\n",
       "count  21000.000000     2.100000e+04  2.100000e+04  21000.000000   \n",
       "mean    1492.403982     5.439214e+08  8.512000e+08     28.612404   \n",
       "std     2711.496882     6.288022e+07  7.824340e+07      3.972833   \n",
       "min        0.000000     4.130518e+08  6.964015e+08     16.731034   \n",
       "25%        8.272388     5.090553e+08  7.890880e+08     25.374816   \n",
       "50%      158.436000     5.512000e+08  8.649196e+08     28.479272   \n",
       "75%     1774.793475     5.893715e+08  9.005551e+08     31.568405   \n",
       "max    22526.610000     6.700157e+08  1.049869e+09     45.290476   \n",
       "\n",
       "       price_regular  price_actual      discount  avg_population_2017  \\\n",
       "count   21000.000000  21000.000000  21000.000000         2.100000e+04   \n",
       "mean     1451.536344   1267.347450    184.374146         1.045065e+06   \n",
       "std       683.362417    587.757323    257.469968         9.291926e+05   \n",
       "min         0.000000  -3121.690141      0.000000         1.227100e+04   \n",
       "25%      1311.547158   1178.365653     54.935108         6.018900e+04   \n",
       "50%      1495.174592   1324.695705    138.307225         1.232242e+06   \n",
       "75%      1725.652080   1517.311427    272.298630         1.729177e+06   \n",
       "max     19166.625000   4925.404000  19166.625000         3.137874e+06   \n",
       "\n",
       "       avg_yearly_household_income_2017  discount_in_percent   timeseries  \\\n",
       "count                      21000.000000         21000.000000  21000.00000   \n",
       "mean                      151073.494286            10.574884    174.50000   \n",
       "std                        50409.593114             9.590813    101.03829   \n",
       "min                        90240.000000             0.000000      0.00000   \n",
       "25%                       110057.000000             3.749628     87.00000   \n",
       "50%                       131411.000000             8.948990    174.50000   \n",
       "75%                       206553.000000            15.647058    262.00000   \n",
       "max                       247220.000000           226.740147    349.00000   \n",
       "\n",
       "           time_idx    log_volume  avg_volume_by_sku  avg_volume_by_agency  \n",
       "count  21000.000000  21000.000000       21000.000000          21000.000000  \n",
       "mean      29.500000      2.464118        1492.403982           1492.403982  \n",
       "std       17.318515      8.178218        1051.790829           1328.239698  \n",
       "min        0.000000    -18.420681           0.000000              0.000000  \n",
       "25%       14.750000      2.112923         932.285496            113.420250  \n",
       "50%       29.500000      5.065351        1402.305264           1730.529771  \n",
       "75%       44.250000      7.481439        2195.362302           2595.316500  \n",
       "max       59.000000     10.022453        4332.363750           5884.717375  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43d35a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>sku</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>...</th>\n",
       "      <th>football_gold_cup</th>\n",
       "      <th>beer_capital</th>\n",
       "      <th>music_fest</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>month</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>avg_volume_by_sku</th>\n",
       "      <th>avg_volume_by_agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agency_22</td>\n",
       "      <td>SKU_01</td>\n",
       "      <td>52.2720</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>25.845238</td>\n",
       "      <td>1168.903668</td>\n",
       "      <td>1069.166193</td>\n",
       "      <td>99.737475</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>8.532566</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.956461</td>\n",
       "      <td>2613.377501</td>\n",
       "      <td>103.805460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Agency_37</td>\n",
       "      <td>SKU_04</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>26.505000</td>\n",
       "      <td>1852.273642</td>\n",
       "      <td>1611.466298</td>\n",
       "      <td>240.807344</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>13.000635</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-18.420681</td>\n",
       "      <td>1361.511918</td>\n",
       "      <td>0.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Agency_59</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>812.9214</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>22.219737</td>\n",
       "      <td>1270.795012</td>\n",
       "      <td>1197.184260</td>\n",
       "      <td>73.610752</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>5.792496</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.700634</td>\n",
       "      <td>1225.306376</td>\n",
       "      <td>2041.909586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Agency_11</td>\n",
       "      <td>SKU_01</td>\n",
       "      <td>316.4400</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>25.360000</td>\n",
       "      <td>1176.155397</td>\n",
       "      <td>1082.757488</td>\n",
       "      <td>93.397909</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>7.940950</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.757134</td>\n",
       "      <td>2613.377501</td>\n",
       "      <td>125.690220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Agency_05</td>\n",
       "      <td>SKU_05</td>\n",
       "      <td>420.9093</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>24.079012</td>\n",
       "      <td>1327.003396</td>\n",
       "      <td>1207.822992</td>\n",
       "      <td>119.180404</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>8.981168</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.042417</td>\n",
       "      <td>1179.728165</td>\n",
       "      <td>1638.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6765</th>\n",
       "      <td>Agency_08</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>9.8136</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>618073219</td>\n",
       "      <td>919709619</td>\n",
       "      <td>25.373665</td>\n",
       "      <td>1706.410263</td>\n",
       "      <td>1455.262060</td>\n",
       "      <td>251.148203</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>14.717926</td>\n",
       "      <td>336</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>2.283769</td>\n",
       "      <td>2304.827516</td>\n",
       "      <td>76.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6764</th>\n",
       "      <td>Agency_60</td>\n",
       "      <td>SKU_05</td>\n",
       "      <td>2235.3495</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>618073219</td>\n",
       "      <td>919709619</td>\n",
       "      <td>23.081069</td>\n",
       "      <td>1898.981558</td>\n",
       "      <td>1528.616113</td>\n",
       "      <td>370.365445</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>19.503372</td>\n",
       "      <td>188</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>7.712153</td>\n",
       "      <td>1530.930920</td>\n",
       "      <td>3311.367493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6763</th>\n",
       "      <td>Agency_19</td>\n",
       "      <td>SKU_05</td>\n",
       "      <td>87.5430</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>618073219</td>\n",
       "      <td>919709619</td>\n",
       "      <td>27.432590</td>\n",
       "      <td>1902.160687</td>\n",
       "      <td>1547.299733</td>\n",
       "      <td>354.860954</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>18.655677</td>\n",
       "      <td>162</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>4.472130</td>\n",
       "      <td>1530.930920</td>\n",
       "      <td>56.557950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>Agency_60</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>325.8792</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>618073219</td>\n",
       "      <td>919709619</td>\n",
       "      <td>23.081069</td>\n",
       "      <td>1704.503815</td>\n",
       "      <td>1444.443913</td>\n",
       "      <td>260.059902</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>15.257220</td>\n",
       "      <td>187</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>5.786527</td>\n",
       "      <td>2304.827516</td>\n",
       "      <td>3311.367493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>Agency_56</td>\n",
       "      <td>SKU_01</td>\n",
       "      <td>3283.8480</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>618073219</td>\n",
       "      <td>919709619</td>\n",
       "      <td>21.709841</td>\n",
       "      <td>1729.148426</td>\n",
       "      <td>1223.228147</td>\n",
       "      <td>505.920279</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>29.258349</td>\n",
       "      <td>71</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>8.096771</td>\n",
       "      <td>2716.823019</td>\n",
       "      <td>3304.106100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         agency     sku     volume       date  industry_volume  soda_volume  \\\n",
       "0     Agency_22  SKU_01    52.2720 2013-01-01        492612703    718394219   \n",
       "238   Agency_37  SKU_04     0.0000 2013-01-01        492612703    718394219   \n",
       "237   Agency_59  SKU_03   812.9214 2013-01-01        492612703    718394219   \n",
       "236   Agency_11  SKU_01   316.4400 2013-01-01        492612703    718394219   \n",
       "235   Agency_05  SKU_05   420.9093 2013-01-01        492612703    718394219   \n",
       "...         ...     ...        ...        ...              ...          ...   \n",
       "6765  Agency_08  SKU_03     9.8136 2017-12-01        618073219    919709619   \n",
       "6764  Agency_60  SKU_05  2235.3495 2017-12-01        618073219    919709619   \n",
       "6763  Agency_19  SKU_05    87.5430 2017-12-01        618073219    919709619   \n",
       "6771  Agency_60  SKU_03   325.8792 2017-12-01        618073219    919709619   \n",
       "6650  Agency_56  SKU_01  3283.8480 2017-12-01        618073219    919709619   \n",
       "\n",
       "      avg_max_temp  price_regular  price_actual    discount  ...  \\\n",
       "0        25.845238    1168.903668   1069.166193   99.737475  ...   \n",
       "238      26.505000    1852.273642   1611.466298  240.807344  ...   \n",
       "237      22.219737    1270.795012   1197.184260   73.610752  ...   \n",
       "236      25.360000    1176.155397   1082.757488   93.397909  ...   \n",
       "235      24.079012    1327.003396   1207.822992  119.180404  ...   \n",
       "...            ...            ...           ...         ...  ...   \n",
       "6765     25.373665    1706.410263   1455.262060  251.148203  ...   \n",
       "6764     23.081069    1898.981558   1528.616113  370.365445  ...   \n",
       "6763     27.432590    1902.160687   1547.299733  354.860954  ...   \n",
       "6771     23.081069    1704.503815   1444.443913  260.059902  ...   \n",
       "6650     21.709841    1729.148426   1223.228147  505.920279  ...   \n",
       "\n",
       "      football_gold_cup  beer_capital music_fest discount_in_percent  \\\n",
       "0                     -             -          -            8.532566   \n",
       "238                   -             -          -           13.000635   \n",
       "237                   -             -          -            5.792496   \n",
       "236                   -             -          -            7.940950   \n",
       "235                   -             -          -            8.981168   \n",
       "...                 ...           ...        ...                 ...   \n",
       "6765                  -             -          -           14.717926   \n",
       "6764                  -             -          -           19.503372   \n",
       "6763                  -             -          -           18.655677   \n",
       "6771                  -             -          -           15.257220   \n",
       "6650                  -             -          -           29.258349   \n",
       "\n",
       "     timeseries time_idx month log_volume avg_volume_by_sku  \\\n",
       "0             0        0     1   3.956461       2613.377501   \n",
       "238           5        0     1 -18.420681       1361.511918   \n",
       "237           9        0     1   6.700634       1225.306376   \n",
       "236          14        0     1   5.757134       2613.377501   \n",
       "235          22        0     1   6.042417       1179.728165   \n",
       "...         ...      ...   ...        ...               ...   \n",
       "6765        336       59    12   2.283769       2304.827516   \n",
       "6764        188       59    12   7.712153       1530.930920   \n",
       "6763        162       59    12   4.472130       1530.930920   \n",
       "6771        187       59    12   5.786527       2304.827516   \n",
       "6650         71       59    12   8.096771       2716.823019   \n",
       "\n",
       "     avg_volume_by_agency  \n",
       "0              103.805460  \n",
       "238              0.549900  \n",
       "237           2041.909586  \n",
       "236            125.690220  \n",
       "235           1638.463500  \n",
       "...                   ...  \n",
       "6765            76.037400  \n",
       "6764          3311.367493  \n",
       "6763            56.557950  \n",
       "6771          3311.367493  \n",
       "6650          3304.106100  \n",
       "\n",
       "[21000 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0a532",
   "metadata": {},
   "source": [
    "### Create Dataset and Dataeloaders\n",
    "\n",
    "The next step is to convert the dataframe into a PyTorch Forecasting TimeSeriesDataSet. Apart from telling the dataset which features are categorical vs continuous and which are static vs varying in time, we also have to decide how we normalise the data. Here, we standard scale each time series separately and indicate that values are always positive. Generally, the EncoderNormalizer, that scales dynamically on each encoder sequence as you train, is preferred to avoid look-ahead bias induced by normalisation. However, you might accept look-ahead bias if you are having troubles to find a reasonably stable normalisation, for example, because there are a lot of zeros in your data. Or you expect a more stable normalization in inference. In the later case, you ensure that you do not learn “weird” jumps that will not be present when running inference, thus training on a more realistic data set.\n",
    "\n",
    "We also choose to use the last six months as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9ac9295",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fc64a",
   "metadata": {},
   "source": [
    "### Create Baselin Model\n",
    "\n",
    "Evaluating a <b>Baseline</b> model that predicts the next 6 months by simply repeating the last observed volume gives us a simle benchmark that we want to outperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b54b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293.0088195800781"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155bc28",
   "metadata": {},
   "source": [
    "### Train the Temporal Fusion Transformer\n",
    "\n",
    "It is now time to create our <b>TemporalFusionTransformer</b> model. We train the model with PyTorch Lightning.\n",
    "\n",
    "### Find Optimal Learning Rate\n",
    "\n",
    "Prior to training, you can identify the optimal learning rate with the <b>PyTorch Lightning learning rate finder</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac2b9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 29.7k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d9c0119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr: 100%|███████████████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.33it/s]Restoring states from the checkpoint path at /Users/Dom/Repos/DeepLearnings/TFTwalkthrough/.lr_find_4e5730f3-23df-4822-b42a-e300695b070c.ckpt\n",
      "Finding best initial lr: 100%|███████████████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 0.01862087136662867\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b6637",
   "metadata": {},
   "source": [
    "For the <b>TemporalFusionTransformer</b>, the optimal learning rate seems to be slightly lower than the suggested one. Further, we do not directly want to use the suggested learning rate because PyTorch Lightning sometimes can get confused by the noise at lower learning rates and suggests rates far too low. Manual control is essential. We decide to pick 0.03 as learning rate.\n",
    "\n",
    "### Train Model\n",
    "\n",
    "If you have troubles training the model and get an error `AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem'`, consider either uninstalling tensorflow or first execute\n",
    "\n",
    "`import tensorflow as tf import tensorboard as tb tf.io.gfile = tb.compat.tensorflow_stub.io.gfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c77ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 29.7k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45184002",
   "metadata": {},
   "source": [
    "Training takes a couple of minutes on my Macbook but for larger networks and datasets, it can take hours. The training speed is here mostly determined by overhead and choosing a larger `batch_size` or `hidden_size` (i.e. network size) does not slow does training linearly making training on large datasets feasible. During training, we can monitor the tensorboard which can be spun up with `tensorboard --logdir=lightning_logs`. For example, we can monitor examples predictions on the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34abd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: lightning_logs/lightning_logs\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "3  | prescalers                         | ModuleDict                      | 256   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 119   \n",
      "----------------------------------------------------------------------------------------\n",
      "29.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "29.7 K    Total params\n",
      "0.119     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  97%|█████████████████████████████████████████▌ | 30/31 [00:07<00:00,  4.12it/s, loss=110, v_num=0, train_loss_step=63.80]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|███████████████████████████| 31/31 [00:08<00:00,  3.72it/s, loss=110, v_num=0, train_loss_step=63.80, val_loss=125.0]\u001b[A\n",
      "Epoch 1:  97%|█▉| 30/31 [00:16<00:00,  1.81it/s, loss=76.5, v_num=0, train_loss_step=76.40, val_loss=125.0, train_loss_epoch=131.0]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██| 31/31 [00:17<00:00,  1.76it/s, loss=76.5, v_num=0, train_loss_step=76.40, val_loss=103.0, train_loss_epoch=131.0]\u001b[A\n",
      "Epoch 2:  97%|█▉| 30/31 [00:25<00:00,  1.16it/s, loss=69.1, v_num=0, train_loss_step=76.30, val_loss=103.0, train_loss_epoch=77.10]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██| 31/31 [00:26<00:00,  1.15it/s, loss=69.1, v_num=0, train_loss_step=76.30, val_loss=102.0, train_loss_epoch=77.10]\u001b[A\n",
      "Epoch 3:  97%|█▉| 30/31 [00:35<00:01,  1.20s/it, loss=65.6, v_num=0, train_loss_step=81.60, val_loss=102.0, train_loss_epoch=71.40]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██| 31/31 [00:37<00:00,  1.20s/it, loss=65.6, v_num=0, train_loss_step=81.60, val_loss=101.0, train_loss_epoch=71.40]\u001b[A\n",
      "Epoch 4:  97%|█▉| 30/31 [00:47<00:01,  1.58s/it, loss=68.2, v_num=0, train_loss_step=75.30, val_loss=101.0, train_loss_epoch=66.40]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██| 31/31 [00:48<00:00,  1.57s/it, loss=68.2, v_num=0, train_loss_step=75.30, val_loss=101.0, train_loss_epoch=66.40]\u001b[A\n",
      "Epoch 5:  97%|█▉| 30/31 [00:58<00:01,  1.96s/it, loss=62.7, v_num=0, train_loss_step=66.10, val_loss=101.0, train_loss_epoch=68.00]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██| 31/31 [01:00<00:00,  1.94s/it, loss=62.7, v_num=0, train_loss_step=66.10, val_loss=103.0, train_loss_epoch=68.00]\u001b[A\n",
      "Epoch 6:  97%|███▊| 30/31 [01:09<00:02,  2.33s/it, loss=63, v_num=0, train_loss_step=51.30, val_loss=103.0, train_loss_epoch=63.30]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|████| 31/31 [01:11<00:00,  2.30s/it, loss=63, v_num=0, train_loss_step=51.30, val_loss=97.10, train_loss_epoch=63.30]\u001b[A\n",
      "Epoch 7:  97%|█▉| 30/31 [01:19<00:02,  2.66s/it, loss=55.3, v_num=0, train_loss_step=48.90, val_loss=97.10, train_loss_epoch=62.40]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██| 31/31 [01:20<00:00,  2.61s/it, loss=55.3, v_num=0, train_loss_step=48.90, val_loss=83.60, train_loss_epoch=62.40]\u001b[A\n",
      "Epoch 8:  97%|█▉| 30/31 [01:29<00:02,  2.98s/it, loss=57.7, v_num=0, train_loss_step=48.00, val_loss=83.60, train_loss_epoch=56.70]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██| 31/31 [01:30<00:00,  2.92s/it, loss=57.7, v_num=0, train_loss_step=48.00, val_loss=83.00, train_loss_epoch=56.70]\u001b[A\n",
      "Epoch 9:  97%|███▊| 30/31 [01:38<00:03,  3.29s/it, loss=54, v_num=0, train_loss_step=61.80, val_loss=83.00, train_loss_epoch=57.10]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|████| 31/31 [01:39<00:00,  3.21s/it, loss=54, v_num=0, train_loss_step=61.80, val_loss=83.30, train_loss_epoch=57.10]\u001b[A\n",
      "Epoch 10:  97%|▉| 30/31 [01:48<00:03,  3.61s/it, loss=51.6, v_num=0, train_loss_step=48.50, val_loss=83.30, train_loss_epoch=54.10]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 31/31 [01:49<00:00,  3.53s/it, loss=51.6, v_num=0, train_loss_step=48.50, val_loss=81.20, train_loss_epoch=54.10]\u001b[A\n",
      "Epoch 11:  97%|▉| 30/31 [01:57<00:03,  3.93s/it, loss=53.5, v_num=0, train_loss_step=51.90, val_loss=81.20, train_loss_epoch=52.90]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|█| 31/31 [01:58<00:00,  3.83s/it, loss=53.5, v_num=0, train_loss_step=51.90, val_loss=80.40, train_loss_epoch=52.90]\u001b[A\n",
      "Epoch 12:  97%|▉| 30/31 [02:06<00:04,  4.23s/it, loss=52.7, v_num=0, train_loss_step=52.00, val_loss=80.40, train_loss_epoch=53.20]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 31/31 [02:07<00:00,  4.12s/it, loss=52.7, v_num=0, train_loss_step=52.00, val_loss=75.70, train_loss_epoch=53.20]\u001b[A\n",
      "Epoch 13:  97%|▉| 30/31 [02:15<00:04,  4.53s/it, loss=52.9, v_num=0, train_loss_step=48.40, val_loss=75.70, train_loss_epoch=51.30]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|█| 31/31 [02:16<00:00,  4.42s/it, loss=52.9, v_num=0, train_loss_step=48.40, val_loss=82.40, train_loss_epoch=51.30]\u001b[A\n",
      "Epoch 14:  97%|▉| 30/31 [02:25<00:04,  4.85s/it, loss=49.2, v_num=0, train_loss_step=55.70, val_loss=82.40, train_loss_epoch=52.80]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|█| 31/31 [02:26<00:00,  4.72s/it, loss=49.2, v_num=0, train_loss_step=55.70, val_loss=79.10, train_loss_epoch=52.80]\u001b[A\n",
      "Epoch 15:  97%|▉| 30/31 [02:34<00:05,  5.15s/it, loss=48.7, v_num=0, train_loss_step=63.00, val_loss=79.10, train_loss_epoch=49.80]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|█| 31/31 [02:35<00:00,  5.01s/it, loss=48.7, v_num=0, train_loss_step=63.00, val_loss=78.30, train_loss_epoch=49.80]\u001b[A\n",
      "Epoch 16:  97%|▉| 30/31 [02:43<00:05,  5.45s/it, loss=46.7, v_num=0, train_loss_step=53.70, val_loss=78.30, train_loss_epoch=49.20]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|█| 31/31 [02:44<00:00,  5.30s/it, loss=46.7, v_num=0, train_loss_step=53.70, val_loss=77.60, train_loss_epoch=49.20]\u001b[A\n",
      "Epoch 17:  97%|▉| 30/31 [02:52<00:05,  5.75s/it, loss=47.3, v_num=0, train_loss_step=44.50, val_loss=77.60, train_loss_epoch=49.80]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|█| 31/31 [02:53<00:00,  5.61s/it, loss=47.3, v_num=0, train_loss_step=44.50, val_loss=77.10, train_loss_epoch=49.80]\u001b[A\n",
      "Epoch 18:  97%|▉| 30/31 [03:02<00:06,  6.07s/it, loss=49.3, v_num=0, train_loss_step=47.90, val_loss=77.10, train_loss_epoch=48.00]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|█| 31/31 [03:03<00:00,  5.90s/it, loss=49.3, v_num=0, train_loss_step=47.90, val_loss=78.00, train_loss_epoch=48.00]\u001b[A\n",
      "Epoch 19:  97%|▉| 30/31 [03:11<00:06,  6.37s/it, loss=46.8, v_num=0, train_loss_step=39.90, val_loss=78.00, train_loss_epoch=47.80]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 31/31 [03:12<00:00,  6.20s/it, loss=46.8, v_num=0, train_loss_step=39.90, val_loss=76.70, train_loss_epoch=47.80]\u001b[A\n",
      "Epoch 20:  97%|▉| 30/31 [03:20<00:06,  6.67s/it, loss=47.4, v_num=0, train_loss_step=58.70, val_loss=76.70, train_loss_epoch=47.30]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|█| 31/31 [03:21<00:00,  6.49s/it, loss=47.4, v_num=0, train_loss_step=58.70, val_loss=77.30, train_loss_epoch=47.30]\u001b[A\n",
      "Epoch 21:  97%|▉| 30/31 [03:29<00:06,  6.98s/it, loss=47.2, v_num=0, train_loss_step=39.20, val_loss=77.30, train_loss_epoch=47.70]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|█| 31/31 [03:30<00:00,  6.79s/it, loss=47.2, v_num=0, train_loss_step=39.20, val_loss=77.30, train_loss_epoch=47.70]\u001b[A\n",
      "Epoch 22:  97%|▉| 30/31 [03:39<00:07,  7.32s/it, loss=46.1, v_num=0, train_loss_step=42.10, val_loss=77.30, train_loss_epoch=46.80]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|█| 31/31 [03:40<00:00,  7.11s/it, loss=46.1, v_num=0, train_loss_step=42.10, val_loss=76.90, train_loss_epoch=46.80]\u001b[A\n",
      "Epoch 22: 100%|█| 31/31 [03:41<00:00,  7.13s/it, loss=46.1, v_num=0, train_loss_step=42.10, val_loss=76.90, train_loss_epoch=46.30]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa642e",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Hyperparamter tuning with [optuna](https://optuna.org/) is directly build into pytorch-forecasting. For example, we can use the <b>optimize_hyperparameters()</b> function to optimize the TFT’s hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f440cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-01 02:22:58,009]\u001b[0m A new study created in memory with name: no-name-cb776226-1103-4071-b682-c37b2a8da935\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:29:07,596]\u001b[0m Trial 0 finished with value: 80.01937866210938 and parameters: {'gradient_clip_val': 0.0421502818287601, 'hidden_size': 12, 'dropout': 0.10778725332389275, 'hidden_continuous_size': 9, 'attention_head_size': 1, 'learning_rate': 0.024189987799770565}. Best is trial 0 with value: 80.01937866210938.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:35:29,504]\u001b[0m Trial 1 finished with value: 89.81295013427734 and parameters: {'gradient_clip_val': 0.010634132967582402, 'hidden_size': 12, 'dropout': 0.14554255118946072, 'hidden_continuous_size': 10, 'attention_head_size': 4, 'learning_rate': 0.0545506823225586}. Best is trial 0 with value: 80.01937866210938.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:35:47,915]\u001b[0m Trial 2 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:36:03,612]\u001b[0m Trial 3 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:36:20,847]\u001b[0m Trial 4 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:36:50,238]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:39:25,394]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 16.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:40:15,292]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 4.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:40:29,582]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:48:01,215]\u001b[0m Trial 9 finished with value: 84.2064437866211 and parameters: {'gradient_clip_val': 0.011344010484200757, 'hidden_size': 29, 'dropout': 0.1187631663501321, 'hidden_continuous_size': 22, 'attention_head_size': 1, 'learning_rate': 0.09009161387595328}. Best is trial 0 with value: 80.01937866210938.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:48:14,298]\u001b[0m Trial 10 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:48:30,840]\u001b[0m Trial 11 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-01 02:49:01,968]\u001b[0m Trial 12 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:49:19,992]\u001b[0m Trial 13 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:49:44,308]\u001b[0m Trial 14 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:50:20,717]\u001b[0m Trial 15 finished with value: 103.15507507324219 and parameters: {'gradient_clip_val': 0.02412333925742853, 'hidden_size': 117, 'dropout': 0.22420382315752135, 'hidden_continuous_size': 58, 'attention_head_size': 1, 'learning_rate': 0.010657256525589475}. Best is trial 0 with value: 80.01937866210938.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:50:37,502]\u001b[0m Trial 16 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2022-05-01 02:50:52,358]\u001b[0m Trial 17 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:171: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[33m[W 2022-05-01 02:50:56,960]\u001b[0m Trial 18 failed because of the following error: KeyError('val_loss')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/Users/Dom/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py\", line 212, in objective\n",
      "    return metrics_callback.metrics[-1][\"val_loss\"].item()\n",
      "KeyError: 'val_loss'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemporal_fusion_transformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize_hyperparameters\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# create study\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptuna_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_clip_val_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_continuous_size_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_head_size_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlimit_train_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_on_plateau_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_learning_rate_finder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use Optuna to find ideal learning rate or use in-built learning rate finder\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# save study results - also we can resume tuning at a later point in time\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_study.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fout:\n",
      "File \u001b[0;32m~/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:217\u001b[0m, in \u001b[0;36moptimize_hyperparameters\u001b[0;34m(train_dataloaders, val_dataloaders, model_path, max_epochs, n_trials, timeout, gradient_clip_val_range, hidden_size_range, hidden_continuous_size_range, attention_head_size_range, dropout_range, learning_rate_range, use_learning_rate_finder, trainer_kwargs, log_dir, study, verbose, pruner, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m study \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner)\n\u001b[0;32m--> 217\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "File \u001b[0;32m~/Repos/enviorments/deepforecast/lib/python3.8/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/enviorments/deepforecast/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/Repos/enviorments/deepforecast/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/enviorments/deepforecast/lib/python3.8/site-packages/optuna/study/_optimize.py:264\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\n",
      "File \u001b[0;32m~/Repos/enviorments/deepforecast/lib/python3.8/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/Repos/enviorments/deepforecast/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:212\u001b[0m, in \u001b[0;36moptimize_hyperparameters.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    209\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# report result\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetrics_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc3460",
   "metadata": {},
   "source": [
    "### Evaluate Performance\n",
    "\n",
    "PyTorch Lightning automatically checkpoints training and thus, we can easily retrieve the best model and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d25bb",
   "metadata": {},
   "source": [
    "After training, we can make predictions with <b>predict()</b>. The method allows very fine-grained control over what it returns so that, for example, you can easily match predictions to your pandas dataframe. See its documentation for details. We evaluate the metrics on the validation dataset and a couple of examples to see how well the model is doing. Given that we work with only 21 000 samples the results are very reassuring and can compete with results by a gradient booster. We also perform better than the baseline model. Given the noisy data, this is not trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c3ee5",
   "metadata": {},
   "source": [
    "We can now also look at sample predictions directly which we plot with <b>plot_prediction()</b>. As you can see from the figures below, forecasts look rather accurate. If you wonder, the grey lines denote the amount of attention the model pays to different points in time when making the prediction. This is a special feature of the Temporal Fusion Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e65f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c72841",
   "metadata": {},
   "source": [
    "### Worst Performers\n",
    "\n",
    "Looking at the worst performers, for example in terms of SMAPE, gives us an idea where the model has issues with forecasting reliably. These examples can provide important pointers about how to improve the model. This kind of actuals vs predictions plots are available to all models. Of course, it is also sensible to employ additional metrics, such as MASE, defined in the metrics module. However, for the sake of demonstration, we only use SMAPE here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte metric by which to display\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "mean_losses = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
    "indices = mean_losses.argsort(descending=True)  # sort losses\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(\n",
    "        x, raw_predictions, idx=indices[idx], add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles)\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566bd65",
   "metadata": {},
   "source": [
    "### Actuals vs predictions by variables\n",
    "\n",
    "Checking how the model performs across different slices of the data allows us to detect weaknesses. Plotted below are the means of predictions vs actuals across each variable divided into 100 bins using the Now, we can directly predict on the generated data using the calculate_prediction_actual_by_variable() and plot_prediction_actual_by_variable() methods. The gray bars denote the frequency of the variable by bin, i.e. are a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f72442",
   "metadata": {},
   "source": [
    "### Predict on selected data\n",
    "\n",
    "To predict on a subset of data we can filter the subsequences in a dataset using the <b>filter()</b> method. Here we predict for the subsequence in the `training` dataset that maps to the group ids “Agency_01” and “SKU_01” and whose first predicted value corresponds to the time index “15”. We output all seven quantiles. This means we expect a tensor of shape `1 x n_timesteps x n_quantiles = 1 x 6 x 7` as we predict for a single subsequence six time steps ahead and 7 quantiles for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a5ddf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
